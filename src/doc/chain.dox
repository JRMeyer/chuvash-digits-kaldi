// doc/chain.dox

// Copyright 2015   Johns Hopkins University (author: Daniel Povey)

// See ../../COPYING for clarification regarding multiple authors
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at

//  http://www.apache.org/licenses/LICENSE-2.0

// THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED
// WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,
// MERCHANTABLITY OR NON-INFRINGEMENT.
// See the Apache 2 License for the specific language governing permissions and
// limitations under the License.

namespace kaldi {

/**
  \page chain 'Chain' models

  \section chain_intro Introduction to 'chain' models

  The 'chain' models are a type of DNN-HMM model, and differ from the
  conventional model in various ways; you can think of them as a different
  design point in the space of acoustic models.

   - We use a 3 times smaller frame rate at the output of the neural net,
     This significantly reduces the amount of computation required in
     test time, making real-time decoding much easier.
   - The models are trained right from the start with a sequence-level
     objective function-- namely, the log probability of the correct sequence.  It is
     essentially MMI implemented without lattices on the GPU, by doing a full
     forward-backward on a decoding graph derived from a phone n-gram language
     model.
   - Because of the reduced frame rate, we need to use unconventional
     HMM topologies (allowing the traversal of the HMM in one state).
   - We don't use any transition probabilities in the HMMs other than those
     derived from the language model (the transition probabilities
     can just be folded into the neural-net output probabilities, so there
     is no need for them).
   - Currently, only nnet3 DNNs are supported (see \ref dnn3), and
     online decoding has not yet been implemented.
   - Currently the results are about the same as those of conventional
     DNN-HMMs, but the system is about 3 times faster to decode.

  \section chain_scripts  Where to find scripts for the 'chain' models

  The current best scripts for the 'chain' models can be found in the
  Switchboard setup in egs/swbd/s5c; the script local/chain/run_tdnn_q.sh is the
  current best one.  This is currently available in the 'chain' branch of Dan's
  git repository (git@github.com:/danpovey/kaldi.git) but will soon be pushed to
  the 'chain' branch of the official Kaldi git repository
  (https://github.com/kaldi-asr/kaldi.git) and eventually will be merged to the
  master.

  This script uses TDNNs as the neural net (we've been doing the development
  with TDNNs because they are easier to tune then LSTMs), and gives almost the same
  WER as the baseline TDNN: 12.1\%, versus 11.8\% for the best TDNN baseline 
  (on the Switchboard-only portion of eval2000).

  \section chain_model  The chain model

  The chain model itself is no different from a conventional DNN-HMM, except we
  don't make use of the transition probabilities in the HMMs (imagine that the
  transition probabilities are always 1.0).  The difference is the objective
  function used to train it: instead of a frame-level objective, we use the
  log-probability of the correct phone sequence as the objective function.  The
  training process is quite similar to MMI training, in which we compute numerator
  and denominator 'occupation probabilities' and the difference between the two
  is used in the derivative computation.  There is no need to normalize the DNN
  outputs to sum to one on each frame any more.


  As mentioned, we configure the model differently from normal: we use a 3 times
  reduced frame rate (so one frame every 30 ms), and modified HMM topologies (we
  are still experimenting with this, so details later).

 

  


     
  

  This page describes the support for Connectionist Temporal Classifiction (CTC)
  in Kaldi.  At the current time CTC is only supported in the nnet3 setup, but
  the non-neural-net parts of the CTC code are designed to be independent of the
  neural net implementation so porting it to other setups such as nnet1 and nnet2
  should be possible.

  The original CTC paper is "Connectionist temporal classification: labelling
  unsegmented sequence data with recurrent neural networks" by Alex Graves et
  al.  The core of CTC is indepenent of of the exact network topology but it has
  traditionally been implemented with recurrent networks such as LSTMs.
  Originally the idea of CTC was to have the network output words directly, but
  more recent work by Google, e.g.  "Fast and Accurate Recurrent Neural Network
  Acoustic Models for Speech Recognition" by Hasim Sak, Andrew Senior et al. has
  made the network output phones, and that's how we are doing it in our
  implementation.  (Our goal is to support applications of speech recognition, and
  while building applications it is generally essential to be able to control the
  language model and vocabulary without rebuilding the model.)

  \section ctc_cctc Context-dependent CTC (CCTC)

  What we have implemented is something we are calling Context-dependent CTC
  (CCTC).  This is not the same as the CD-CTC that already exists in the
  literature, although it does stand for the same sequence of words.
  CCTC is different from phone-level (or triphone-level) CTC in two ways:

    - In CCTC, we get to 'see' the immediate left phonetic context as we
      predict the next phone.
    - In CCTC, in order to force the model to learn only the acoustic
      probabilities we 'train against' a phone-level language model;
      in test time, this is replaced by the probabilities from
      a phone-level decoding graph.

  The reader might object that normal CTC already knows the left phonetic
  context, if it is using a recurrent netowork, so what is different here?
  The difference is that CTC only knows what probabilities it has output;
  it doesn't know for sure what phone was chosen.

  Below, we will explain what CCTC is by working in baby steps from a simpler to a more
  complex system, explaining CTC on the way.

  \subsection ctc_cctc_baseline1 Baseline 1: monophone phone-level neural network

  The simplest starting point is a neural network that outputs monophone
  labels (so the output dimension of the network is the same as the number of phones).  Imagine
  that the network predicts \f$p(q | x)\f$ where q is a phone symbol and x is the acoustic
  features.  When decoding, we use Bayes' rule to write  \f$p(x | q) = p(q | x) p(x) / p(q)\f$,
  and we ignore \f$p(x)\f$ because it won't affect the decoding result, so that the quantity
  we use from the network is $p(q | x) / p(q)$, which can be thought of
  as the posterior of the phone $q$ corrected for the overall frequency of that phone.  This
  gets combined with the language-model probability of the word-sequence, and the language
  model log-probability of the sequence gets scaled by a constant (typically around 10).
  This is a conventional 'hybrid' DNN-HMM speech recognition system, except without
  phonetic context-dependency and using a 1-state HMM for each phone instead of a 3-state HMM.
  Note: in principle the neural network could be any kind of neural network, but it's best
  to imagine that it's an LSTM or bi-directional LSTM (BLSTM).

  \subsection ctc_cctc_baseline2 Baseline 2: monophone badly-tuned CTC system

  Imagine that we took the system from Baseline 1 and added one more label to the neural
  network's output, called 'blank'.  Then imagine that we changed the HMM topology so that
  each phone can generate exactly one copy of its phone symbol, with any number of 'blank'
  symbols allowed between phones.  Also imagine that we decimate the output frames of the
  neural network so that we only have one frame every 30ms, instead of every 10ms (the network
  still gets to see the input at the 10ms frame rate, since we stack the input features; see Sak
  et al.).  When decoding, we don't divide by the phone-level priors.  When training,
  we do forward-backward rather than Viterbi (note: this requires training on whole utterances).

  \subsection ctc_cctc_baseline2alt Baseline 2(alt): monophone better-tuned CTC system

  Baseline 2(alt) is as Baseline 2 except tuning it slightly better.  We separate this
  from Baseline 2 because the problems that are being addressed here are solved in a different
  way in CCTC, so Baseline 2) remains the shortest path to CCTC.
  The tuning improvemets (see Sak et al.) are:
    - Instead of forcing exactly one non-blank symbol per phone, we allow repeats of the
      symbol, as long as there are no blanks between the repeats.
    - We still don't divide by the phone-level priors, but in test time we do
      scale down the probability of the blank symbol by factor that is tuned (e.g. around 10).

  \subsection ctc_cctc_baseline3alt Baseline 3(alt): triphone better-tuned CTC system

  Baseline 3(alt) is as Baseline 2(alt) except it uses context-dependent triphones as targets
  rather than context-dependent phones, and it also reintroduces the language
  model scaling factor (but of around 2, rather than around 10 for a conventional
  HMM-based system).  This is the best system in the Sak et al. paper- but note that
  the triphone context-dependency only gives a very small improvement compared with the monophone
  system: 12.7% to 12.2% word error.

  \subsection ctc_cctc_baseline3  Baseline 3: left-context CTC system

  Baseline 3 is as Baseline 2, but the labels are clustered context-dependent phones
  with only left context-- for example, clustered biphones.  We can get the clustering
  from the same Gaussian-based clustering process that we use for a GMM-based system.

  \subsection ctc_cctc_baseline4  Baseline 4: left-context CTC system with `correct normalization'

  Baseline 4 is as Baseline 3, except it differs in how we normalize the
  probabilities.  In all previous baselines, there was just one space over which
  we normalized the probabilities to sum to one.  Here, we normalize only over those
  phones-in-context that are allowed given the left context.  The left context is obtained
  in decoding time by using a context-dependent decoding graph; in training time it's known from
  the reference phone sequence.  Suppose it's an n-phone left-context system (e.g. n=2 means
  a biphone system).  Let h represent the left-context history, which will be a sequence of
  n-1 phones.  For each phone q we can work out which clustered biphone corresponds to the pair
  (h, q); and the probability of each phone q given that history is normalized over that set.
  In practice, the neural network itself would output the un-normalized log-probabilities, and
  the CTC code would compute the normalized phone probabilities.  Any
  probability \f$p(q | h)\f$ can be written as
    \f[ p(q | h) = p(q, h) / \sum_{q'} p(q', h) \f]
   where the un-normalized log-probabilities \f$ \mathrm{log} p(q, h) \f$ are obtained by computing the corresponding clustered
  context-dependent phone and using that index to perform a lookup in the neural network's output.
  Note: these \f$p(q, h)\f$ quantities are not normalized.
  The \f$p(q | h)\f$ quantities are used in both training and decoding.


  \subsection ctc_cctc_baseline5  Baseline 5: context-dependent blank symbol

  Baseline 5 is a simple extension of Baseline 4, in which, instead of having one blank
  symbol, we have a separate blank symbol for each distinct history state h.  In the
  configuration we envisage, we would only have biphone context, so each left-phone corresponds
  to its own history-state, and the number of blank symbols would be equal to the
  number of phones.

  \subsection ctc_cctc_cctc  CCTC itself: incorporating a phone language model

  Finally we get to CCTC itself.  The difference from Baseline 5 is that prior to training
  the model, we estimate a phone n-gram language model, and we `train against' this.

  This is done in order to discourage the neural net itself from learning a kind
  of language model on phones-- that is what we have our word-based decoding
  graph for.  In test time we rely entirely on the word graph for the language
  model, and don't use the phone n-gram language model.  We can view decoding as
  differing from training by the application of a `correction factor' equal to
  the ratio of the word-graph-based LM probability to the phone-ngram LM
  probability.

  Let us write the phone language model probabilities as \f$p_{lm}(q | h)\f$,
  where h represents the language-model history state.  We use a Kneser-Ney
  n-gram language model for this, but modified using count cutoffs to minimize
  the number of distinct history states we need to keep track of (because
  history-states have a cost in decoding time-- we need to compute a normalizer
  for each one).  You can think of h as representing a sequence of phones.
  In, say, a 4-gram phone language model, the maximum number of phones in h
  is three, but most history-states will be much shorter than that because of data
  sparsity and the count cutoff.

  The key equation in CCTC is
\f[
     p(q | h) = \frac{ p_{ac}(q, h) p_{lm}(q | h)  }
                     { \sum_{q'}    p_{ac}(q', h) p_{lm}(q' | h)   }
\f]
 where \f$p_{lm}(q | h)\f$ represents the phone-level language model probabilities
 and \f$p_{ac}(q, h)\f$ represents the un-normalized clustered context-dependent
 acoustic probabilities that come from the output of the neural network (actually,
 we exponentiate the network's output to get these quantities).
 Don't attempt to make sense of any of this in some kind of Bayesian context.  The
 formula is to be interpreted as defining the model itself.  That is, it defines
 how we train the model.
 The only thing in this formula that represents a well-defined, pre-existing constant
 probability is \f$p_{lm}(q | h)\f$.  \f$\log p(q | h)\f$ is something that we train,
 and the formula defines how we interpret the neural network's outputs.

  \subsection ctc_cctc_decoding Decoding in CCTC: using the `real' language model

 When we decode with the CTC, the only difference from the training setup is that we replace the
 phone n-gram probability \f$p_{lm}(q | h)\f$ in the <em>numerator</em>
 of the equation with the probability from our chosen word-level decoding graph, which we
 can write as \f$p_{\mathrm{graph}}(q | h)\f$.  So we could write the decoding
 equation as:
\f[
     p(q | h) = \frac{ p_{ac}(q, h) p_{\mathrm{graph}}(q | h)  }
                     { \sum_{q'}    p_{ac}(q', h) p_{lm}(q' | h)   }
\f]
 However, this doesn't correspond to how we do the computation.  In practice
 we compute the following quantity:
\f[
     p(q | h) = \frac{ p_{ac}(q, h) }
                     { \sum_{q'}    p_{ac}(q', h) p_{lm}(q' | h)   }
\f]
which we can view as acoustic-only part of the probability, coming from the CCTC code,
and this gets combined with the language model probability \f$p_{\mathrm{graph}}(q | h)\f$
when we do the Viterbi graph search.  All this should be familiar to people with
a speech recognition background.


\subsection ctc_cctc_further Further details of CCTC

Some key details which we glossed over above are:

  - In the equations above, the history-state h is always the most detailed
    history-state that either the phone language-model or the lookup phonetic context
    requires.  We don't want to get into the nitty gritty of how we ensure
    this; but for now just rest assured that all the quantities we need to compute
    as a function of h are well-defined because h is 'sufficiently detailed'.

  - For blank symbols, we always take \f$p_{lm}(\mathrm{blank} | h)\f$ to equal 1.0, in both
    training time and test time.  Here, by `blank' we mean whichever blank symbol
    is appropriate for this history-state.
    This might seem less than ideal, but in fact there is no problem, because the
    equation above *defines* the model, and the model can learn anything it needs
    to learn by modifying the `acoustic' probabilities of the blank symbols.

 \subsection ctc_cctc_tuning Simplifications in CCTC

 When we were leading up to CCTC via a sequence of baselines, you will recall that
 there was an `alternative branch' to Baseline 2(alt) and Baseline 3(alt), that
 we put into a separate path because we don't incorporate those features in CCTC.

 Firstly, of course we don't use fully context-dependent phones, we only use
 left-context.  This is what makes the other concepts work.  We note that the
 difference between monophone and triphone is not that great in Google's paper
 (e.g. 12.7% versus 12.2% word error) so we hope the limitation to left phonetic
 context for the acoustic states won't be a very great limitation.

 Regarding repeats of the phone symbols-- the Sak et al implementation that we
 are following, and the original paper on CTC, allow repetitions of the phone
 symbols as long as they are delineated by blanks (to distinguish the case where
 the underlying symbol really repeats).  Our framework doesn't allow this.  This
 is mostly as a simplification.  The problem the repeated-symbols was trying to
 solve is that sometimes the probabilities of phone symbols can be a little
 spread-out over time-- it can be hard for the network to learn a distribution that
 is very 'spiky'.  The reason we don't think this will be a problem in our setup
 is that the language-model probability of a phone following itself will tend to be quite
 low, so once we see a phone the model will give up trying to model itself again.
 Note: we don't expect that this explanation will make much sense to anyone
 who has not thought long and hard about CTC and the implications of our
 approach.  A more detailed explanation would take a long time to write.

 Regarding the scaling of the probability of the blank symbol, which in the Sak et al
 paper was scaled by a factor of 1/10-- we don't believe this will be necessary in our framework,
 because it happens `naturally'.  In decoding time we omit the factor \f$p_{lm}(q | h)\f$ from
 the numerator of \f$p(q | h)\f$ because it's `accounted for' by the graph.  Assuming the phone-level
 perplexity is about 10, which is a reasonable value, this has a very similar effect to scaling
 the non-blank phone probabilities \em up by a factor 10, which has the same effect as scaling
 blank \em down by a factor of 10.  So we don't believe any further scaling of the blank symbol
 will be necessary.

 \subsection ctc_cctc_tombstone 'Tombstone' CCTC

 An extension to CCTC which seems to be working very well is the 'Tombstone' concept, which
 we will briefly describe here (pending more complete documentation).  We believe this
 will become the standard CTC recipe we use, because the results are so far very encouraging.

 You can view the model we train with in CCTC as a HMM, where the number of HMM
 states is a little bit more than the number of distinct history-states in the
 phone language model (it's slightly more because of phonetic-context
 effects.. it's complicated, just ignore that for now).  Each HMM state has
 num-phones + 1 transitions out of it: one for a blank symbol specific to the
 phonetic history-state (this one is a self-loop), and one each for each of the
 context-independent phones.  The 'tombstone' idea is to add one more transition
 out of each HMM-state: a transition to a 'tombstone state' that doesn't go
 anywhere and isn't part of any valid sequence that we can decode.  If the model takes the 'tombstone'
 transition, it's saying ``I don't like any of these alternatives.  I think a mistake
 was made earlier in the prediction''.  That is, it's saying that it's not happy
 being in the phonetic-LM history state that it is in.

 This allows the CCTC model to atone for earlier mistakes, and it means that it
 doesn't have to wait until all the evidence is in before outputting symbols.
 Instead it can output them 'provisionally' and then later decide it made a bad
 move.  Of course, none of this affects the Viterbi decoding process-- the
 decoding process is exactly the same, and doesn't ever look at the tombstone
 symbols. </em>The tombstones are simply a mechanism to relax the constraint
 that the transitions out of each state sum to one.</em>.  In fact, there are
 other ways to achieve the same thing-- for instance, we could take out the
 denominator of the CCTC equation, which would substantially simplify the
 training and recognition process.  We will have to experiment with things like
 that.

 The 'tombstone' idea is not applicable to conventional CTC,.  Viewing CTC it as
 a special case of CCTC, there is just one HMM state -- imagine there is just
 one phonetic language-model state -- and the effect of the tombstone
 probability would just cancel out and would not affect the probability of different
 sequences differently.


 In normal CCTC, the model is 'locally normalized' like a directed graphical
 model because we ensure via the model structure tthat the
 (acoustic-data-dependent) transition probabilities out of each state sum to
 one.  Tombstone CCTC is more like an undirected graphical model that has
 a Z factor that needs to be computed-- this gives rise to the denominator
 term in training, which we will talk about below.

 \subsection ctc_cctc_tombstone_training Training 'Tombstone' CCTC

 The training process in 'tombstone' CCTC essentially differs from CCTC training
 in the same way as MMI training differs from Maximum Likelihood training, in that
 we add a denominator term to the objective function in addition to the existing
 numerator term.  The denominator does not need a lattice because it is feasible,
 on a GPU card, to do a full forward-backward over the HMM states of the CCTC model.
 (The number of HMM states is, as mentioned before, generally a little larger than
 the number of history-states in the phone-level language model).

 In order to avoid confusion with other uses of the words 'numerator' and 'denominator'
 in CCTC, we refer to the numerator as the 'positive' part and the denominator as the
 'negative' part of the objective function, because the likelihood from
 denominator is negated in the overall objective function.  Both the numerator
 and denominator involve an alpha-beta forward-backward algorithm; however, the numerator
 is constrained by a Finite State Acceptor representation of the allowable sequences
 (we do this so that we can control at what frames any given phone can appear, to within,
 say, a range of 5 frames before the start of the phone in the a reference alignment,
 to 10 frames after the end.  The denominator allows all paths through the HMM without
 any constraint, so you can view the numerator as a subset of the denominator.

 We have to be a little careful with edge effects-- remember that we train on
 split-up pieces of files, of typically 50 to 100 frames.  If we used a fixed
 distribution for the 'initial-probabilities' of the HMM states in the
 denominator, we found that the objective function tended to be positive, due to
 these edge effects-- basically, because the 'good' HMM-states have a low
 initial-probability.  We were able to fix this by assigning the first-frame
 alpha probabilities of the denominator to be the same as those used for the
 numerator, which is a uniform distribution over the small subset of HMM-states
 that are 'allowed' in the FST on the first frame of the numerator FST.  This is
 anyway a closer approximation to what the alpha scores would look like if we
 had done forward-backward over the entire file without breaking it up.  Note
 that we anyway discard the derivatives of the (currently) first 5 and last 10
 frames of each segment of (say) 75 frames that we train on, out of concern that
 the occupation probabilities there are inaccurate due to edge effects.


 \subsection ctc_cctc_tombstone_decoding Decoding using 'Tombstone' CCTC

 We found that with 'tombstone' CCTC, no special tricks are necessary during decoding.
 Adding a penalty on blank is not helpful.  For the language model, we completely
 remove the phone language model that was used during training, and replace
 it with the score from the language-model FST, as described in \ref ctc_cctc_decoding.
 Although our setup supports a language-model scale, in practice it tends to
 choose a scale of one.  Note, in the scripts you'll see values like 9, 10 and 11 being
 chosen, but these have to be divided by 10 to get the true scale: we scale down the
 acoustic probabilities by a factor of 10 in the lattices after decoding so that
 we can test integer language-model scale factors and still get a fine enough gradation in
 relative scales.

  We noticed that the 'tombstone' models, at least in the WSJ setup, tend to
 decode '<UNK>' quite often (about 10 times more frequently than before), and
 that it quite often corresponds to words that were in fact out of vocabulary.
 We tried penalizing '<UNK>' in the decoding graph and, while it reduced the
 number of times that '<UNK>' was decoded, the WER did not improve.
 We regard this as encouraging-- maybe the tombstone model is just better
 at detecting out-of-vocabulary words.


*/


}
